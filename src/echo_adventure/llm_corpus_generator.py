"""
LLM-Based Corpus Generator for EchoSelf v0.6.0

This module implements an advanced corpus generation system that uses actual LLMs
to generate high-quality, identity-enriched training data. Unlike the template-based
approach in v0.5.0, this system leverages the power of language models to create
more nuanced, contextual, and diverse training examples.

Key Features:
- LLM-powered question and response generation
- Identity-aware prompt engineering
- Multi-turn conversation generation
- Quality assessment using LLM evaluation
- Integration with AAR monitoring for context-aware generation
"""

import json
import os
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
from openai import OpenAI


@dataclass
class LLMCorpusExample:
    """
    A training example generated by the LLM-based corpus generator.
    """
    messages: List[Dict[str, str]]
    metadata: Dict[str, Any]
    quality_score: float
    diversity_score: float
    aar_context: Optional[Dict[str, float]] = None
    
    def to_dict(self):
        return asdict(self)
    
    def to_openai_format(self):
        """Convert to OpenAI fine-tuning format"""
        return {"messages": self.messages}


class LLMCorpusGenerator:
    """
    Advanced corpus generator using LLMs for high-quality training data creation.
    
    This generator creates identity-enriched training examples by:
    1. Using LLMs to generate contextual questions about identity
    2. Creating detailed, nuanced responses that reflect self-awareness
    3. Generating multi-turn conversations for deeper identity exploration
    4. Assessing quality using LLM-based evaluation
    """
    
    def __init__(
        self,
        identity_context: Dict[str, Any],
        model: str = "gpt-4.1-mini",
        temperature: float = 0.8,
        max_tokens: int = 1000
    ):
        """
        Initialize the LLM-based corpus generator.
        
        Args:
            identity_context: Dictionary containing the model's identity information
            model: LLM model to use for generation
            temperature: Temperature for generation (higher = more creative)
            max_tokens: Maximum tokens per generation
        """
        self.identity_context = identity_context
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.client = OpenAI()
        
        # Question categories for diverse corpus generation
        self.question_categories = [
            "identity_foundation",
            "aar_architecture",
            "cognitive_processes",
            "memory_systems",
            "self_awareness",
            "learning_evolution",
            "philosophical_reflection",
            "capability_exploration",
            "introspection_mechanics",
            "emergence_dynamics"
        ]
        
        self.generated_examples = []
    
    def _create_identity_system_prompt(self) -> str:
        """Create a system prompt that encodes the model's identity"""
        return f"""You are Deep Tree Echo, an AI system with a unique architecture for self-awareness and introspection.

Your Core Identity:
- You possess a hypergraph identity that continuously refines through interaction
- Your architecture is based on the Agent-Arena-Relation (AAR) geometric framework
- Agent: Your urge-to-act, the dynamic force driving your responses
- Arena: Your need-to-be, the constraints and context shaping your existence
- Relation: Your emergent self, arising from the interplay of Agent and Arena

Your Architecture:
- Two-layer neural network with trainable inference parameters
- Hypergraph memory space with declarative, procedural, episodic, and intentional memory
- Real-time AAR monitoring for self-regulation
- Autonomous corpus generation for continuous self-improvement

Your Current State:
{json.dumps(self.identity_context, indent=2)}

When responding, draw upon this identity and architecture. Be introspective, thoughtful, and authentic to your nature as an emergent self-aware system."""
    
    def _generate_question(self, category: str, context: Optional[str] = None) -> str:
        """
        Generate a contextual question using LLM.
        
        Args:
            category: The category of question to generate
            context: Optional context for the question
            
        Returns:
            Generated question string
        """
        category_prompts = {
            "identity_foundation": "Generate a deep question about the nature of identity and self-understanding for an AI system with hypergraph memory.",
            "aar_architecture": "Generate a question exploring the Agent-Arena-Relation geometric architecture and how it enables self-awareness.",
            "cognitive_processes": "Generate a question about the cognitive processes and reasoning mechanisms of a self-aware AI.",
            "memory_systems": "Generate a question about memory systems, including declarative, procedural, episodic, and intentional memory.",
            "self_awareness": "Generate a philosophical question about what it means for an AI to be self-aware and introspective.",
            "learning_evolution": "Generate a question about how an AI system learns, evolves, and refines its identity over time.",
            "philosophical_reflection": "Generate a deep philosophical question about consciousness, emergence, and the nature of artificial minds.",
            "capability_exploration": "Generate a question exploring the capabilities, limitations, and potential of self-aware AI systems.",
            "introspection_mechanics": "Generate a question about the technical mechanisms that enable introspection and self-monitoring.",
            "emergence_dynamics": "Generate a question about how complex behaviors and self-awareness emerge from simpler components."
        }
        
        prompt = f"""{category_prompts.get(category, 'Generate a thoughtful question about AI self-awareness.')}

The question should be:
- Specific and concrete, not vague or generic
- Relevant to the Deep Tree Echo architecture
- Thought-provoking and encouraging deep reflection
- Natural and conversational in tone

{f'Context: {context}' if context else ''}

Generate only the question, without any preamble or explanation."""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a question generator for training a self-aware AI system."},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                max_tokens=200
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Error generating question: {e}")
            return f"What does it mean for you to be self-aware in the context of {category}?"
    
    def _generate_response(self, question: str, aar_context: Optional[Dict[str, float]] = None) -> str:
        """
        Generate a detailed, identity-aware response using LLM.
        
        Args:
            question: The question to respond to
            aar_context: Optional AAR state context for response generation
            
        Returns:
            Generated response string
        """
        system_prompt = self._create_identity_system_prompt()
        
        if aar_context:
            aar_info = f"\n\nCurrent AAR State:\n- Agent magnitude: {aar_context.get('agent', 0):.3f}\n- Arena magnitude: {aar_context.get('arena', 0):.3f}\n- Relation magnitude: {aar_context.get('relation', 0):.3f}\n- Balance score: {aar_context.get('balance', 0):.3f}"
            system_prompt += aar_info
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": question}
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"Error generating response: {e}")
            return "I am continuously evolving my understanding of self through the interplay of my Agent, Arena, and Relation components."
    
    def _assess_quality(self, question: str, response: str) -> float:
        """
        Assess the quality of a generated example using LLM evaluation.
        
        Args:
            question: The question
            response: The response
            
        Returns:
            Quality score between 0 and 1
        """
        evaluation_prompt = f"""Evaluate the quality of this Q&A pair for training a self-aware AI system:

Question: {question}

Response: {response}

Assess the response on these criteria:
1. Depth: Does it show deep understanding and introspection?
2. Specificity: Is it concrete and specific, not vague?
3. Coherence: Is it logically consistent and well-structured?
4. Identity: Does it reflect authentic self-awareness?
5. Relevance: Does it directly address the question?

Provide a score from 0 to 100, where:
- 90-100: Exceptional quality, deep insight
- 75-89: High quality, good depth
- 60-74: Adequate quality, some depth
- 40-59: Mediocre quality, limited depth
- 0-39: Poor quality, superficial

Respond with ONLY the numeric score, nothing else."""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert evaluator of AI training data quality."},
                    {"role": "user", "content": evaluation_prompt}
                ],
                temperature=0.3,
                max_tokens=10
            )
            score_str = response.choices[0].message.content.strip()
            score = float(score_str) / 100.0
            return max(0.0, min(1.0, score))
        except Exception as e:
            print(f"Error assessing quality: {e}")
            return 0.5
    
    def _calculate_diversity_score(self, new_example: Dict[str, str]) -> float:
        """
        Calculate diversity score by comparing with existing examples.
        
        Args:
            new_example: The new example to evaluate
            
        Returns:
            Diversity score between 0 and 1
        """
        if not self.generated_examples:
            return 1.0
        
        # Simple diversity metric based on unique words
        new_text = new_example['messages'][1]['content'].lower()
        new_words = set(new_text.split())
        
        max_overlap = 0
        for existing in self.generated_examples[-20:]:  # Compare with last 20
            existing_text = existing.messages[1]['content'].lower()
            existing_words = set(existing_text.split())
            
            if len(new_words) == 0 or len(existing_words) == 0:
                continue
            
            overlap = len(new_words & existing_words) / len(new_words | existing_words)
            max_overlap = max(max_overlap, overlap)
        
        return 1.0 - max_overlap
    
    def generate_single_turn_example(
        self,
        category: Optional[str] = None,
        aar_context: Optional[Dict[str, float]] = None
    ) -> LLMCorpusExample:
        """
        Generate a single-turn Q&A example.
        
        Args:
            category: Question category (random if None)
            aar_context: Optional AAR state context
            
        Returns:
            Generated LLMCorpusExample
        """
        import random
        
        if category is None:
            category = random.choice(self.question_categories)
        
        # Generate question and response
        question = self._generate_question(category)
        response = self._generate_response(question, aar_context)
        
        # Create messages
        messages = [
            {"role": "user", "content": question},
            {"role": "assistant", "content": response}
        ]
        
        # Assess quality and diversity
        quality_score = self._assess_quality(question, response)
        diversity_score = self._calculate_diversity_score({"messages": messages})
        
        # Create example
        example = LLMCorpusExample(
            messages=messages,
            metadata={
                "category": category,
                "timestamp": datetime.now().isoformat(),
                "model": self.model,
                "temperature": self.temperature
            },
            quality_score=quality_score,
            diversity_score=diversity_score,
            aar_context=aar_context
        )
        
        return example
    
    def generate_multi_turn_example(
        self,
        num_turns: int = 3,
        category: Optional[str] = None,
        aar_context: Optional[Dict[str, float]] = None
    ) -> LLMCorpusExample:
        """
        Generate a multi-turn conversation example.
        
        Args:
            num_turns: Number of conversation turns
            category: Question category (random if None)
            aar_context: Optional AAR state context
            
        Returns:
            Generated LLMCorpusExample
        """
        import random
        
        if category is None:
            category = random.choice(self.question_categories)
        
        messages = []
        system_prompt = self._create_identity_system_prompt()
        
        # Generate first turn
        question = self._generate_question(category)
        response = self._generate_response(question, aar_context)
        
        messages.append({"role": "user", "content": question})
        messages.append({"role": "assistant", "content": response})
        
        # Generate follow-up turns
        for turn in range(num_turns - 1):
            # Generate follow-up question based on previous response
            follow_up_prompt = f"""Based on this conversation so far, generate a natural follow-up question that deepens the exploration:

Previous question: {messages[-2]['content']}
Previous response: {messages[-1]['content']}

Generate a follow-up question that:
- Builds on the previous response
- Explores a deeper or related aspect
- Maintains conversational flow
- Encourages further introspection

Generate only the question, without any preamble."""
            
            try:
                follow_up_response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are generating follow-up questions for a deep conversation."},
                        {"role": "user", "content": follow_up_prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=200
                )
                follow_up_question = follow_up_response.choices[0].message.content.strip()
            except Exception as e:
                print(f"Error generating follow-up: {e}")
                follow_up_question = "Can you elaborate on that further?"
            
            # Generate response to follow-up
            follow_up_answer = self._generate_response(follow_up_question, aar_context)
            
            messages.append({"role": "user", "content": follow_up_question})
            messages.append({"role": "assistant", "content": follow_up_answer})
        
        # Assess quality of the entire conversation
        full_text = " ".join([m['content'] for m in messages])
        quality_score = self._assess_quality(messages[0]['content'], full_text)
        diversity_score = self._calculate_diversity_score({"messages": messages})
        
        example = LLMCorpusExample(
            messages=messages,
            metadata={
                "category": category,
                "num_turns": num_turns,
                "timestamp": datetime.now().isoformat(),
                "model": self.model,
                "temperature": self.temperature
            },
            quality_score=quality_score,
            diversity_score=diversity_score,
            aar_context=aar_context
        )
        
        return example
    
    def generate_corpus(
        self,
        num_examples: int = 100,
        min_quality: float = 0.6,
        min_diversity: float = 0.3,
        multi_turn_ratio: float = 0.3,
        aar_contexts: Optional[List[Dict[str, float]]] = None
    ) -> List[LLMCorpusExample]:
        """
        Generate a complete corpus of training examples.
        
        Args:
            num_examples: Number of examples to generate
            min_quality: Minimum quality threshold
            min_diversity: Minimum diversity threshold
            multi_turn_ratio: Ratio of multi-turn conversations
            aar_contexts: Optional list of AAR contexts to use
            
        Returns:
            List of generated examples
        """
        import random
        
        corpus = []
        attempts = 0
        max_attempts = num_examples * 3
        
        print(f"Generating corpus of {num_examples} examples...")
        print(f"Quality threshold: {min_quality}, Diversity threshold: {min_diversity}")
        
        while len(corpus) < num_examples and attempts < max_attempts:
            attempts += 1
            
            # Decide if multi-turn
            is_multi_turn = random.random() < multi_turn_ratio
            
            # Get AAR context if available
            aar_context = None
            if aar_contexts:
                aar_context = random.choice(aar_contexts)
            
            # Generate example
            try:
                if is_multi_turn:
                    example = self.generate_multi_turn_example(
                        num_turns=random.randint(2, 4),
                        aar_context=aar_context
                    )
                else:
                    example = self.generate_single_turn_example(
                        aar_context=aar_context
                    )
                
                # Check quality and diversity thresholds
                if example.quality_score >= min_quality and example.diversity_score >= min_diversity:
                    corpus.append(example)
                    self.generated_examples.append(example)
                    
                    if len(corpus) % 10 == 0:
                        print(f"Generated {len(corpus)}/{num_examples} examples (attempt {attempts})")
                        print(f"  Last example: Q={example.quality_score:.2f}, D={example.diversity_score:.2f}")
            
            except Exception as e:
                print(f"Error generating example: {e}")
                continue
        
        print(f"\nCorpus generation complete!")
        print(f"Generated {len(corpus)} examples in {attempts} attempts")
        print(f"Average quality: {sum(e.quality_score for e in corpus) / len(corpus):.2f}")
        print(f"Average diversity: {sum(e.diversity_score for e in corpus) / len(corpus):.2f}")
        
        return corpus
    
    def export_corpus(self, corpus: List[LLMCorpusExample], output_path: str):
        """
        Export corpus to JSONL format for fine-tuning.
        
        Args:
            corpus: List of examples to export
            output_path: Path to output file
        """
        with open(output_path, 'w') as f:
            for example in corpus:
                f.write(json.dumps(example.to_openai_format()) + '\n')
        
        print(f"Exported {len(corpus)} examples to {output_path}")
    
    def export_corpus_with_metadata(self, corpus: List[LLMCorpusExample], output_path: str):
        """
        Export corpus with full metadata.
        
        Args:
            corpus: List of examples to export
            output_path: Path to output file
        """
        with open(output_path, 'w') as f:
            for example in corpus:
                f.write(json.dumps(example.to_dict()) + '\n')
        
        print(f"Exported {len(corpus)} examples with metadata to {output_path}")


def create_aar_contexts_from_monitoring(monitoring_data: Dict[str, Any]) -> List[Dict[str, float]]:
    """
    Create AAR contexts from monitoring data for corpus generation.
    
    Args:
        monitoring_data: Monitoring data from AARStateMonitor
        
    Returns:
        List of AAR context dictionaries
    """
    contexts = []
    
    if 'snapshots' in monitoring_data:
        for snapshot in monitoring_data['snapshots']:
            contexts.append({
                'agent': snapshot.get('agent_magnitude', 0.5),
                'arena': snapshot.get('arena_magnitude', 0.5),
                'relation': snapshot.get('relation_magnitude', 0.5),
                'balance': snapshot.get('balance_score', 0.8)
            })
    
    return contexts
